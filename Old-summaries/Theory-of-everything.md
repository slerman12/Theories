Most theories of everything are probably more mathematically motivated than this one. I can't say, after years of writing maths in my journals and boiling my ideas down to simple nutshells, that I'm too interested in writing a [deduction](https://github.com/animal-tree/Writing-stuff/blob/main/Stuff38-Something-From-Nothing.md) at this moment. It requires covering too much ground over too long a time, and I want to get to the point, the interesting points — consciousness, God, wave-particle duality, search and optimization, and qualia.

Bear with me. This is the first time I've tried to go through it all at once and in a way meant to communicate the ideas to others rather than to myself in my journals. I hope I can do a good job of relaying their beauty and simplicity, and moreover why to some degree, many of them are painfully obvious and intuitive/natural-feeling once realized. To do that in writing about mathy subjects is hard, so forgive me if I have to go over and recover the bases a few times. (in future writings)

In a nutshell, the theory is that the universe is a constrained system optimizing qualia. This theory would seem on the surface to be a description of a kind of God, but what differentiates it from religion is it posits reasons why that intelligence would manifest as it has, today, in the laws of physics. And furthermore where it might be and how to test it, how we might infer to and from it[^4], and how this may all, in the fabric of spacetime, "entangle" into our own higher experience of human consciousness.

Views such as panpsychism propose intelligence — or rather, consciousness — at the fabric of spacetime. The angle that this thesis takes is more about intelligence, from the perspective of AI, modern deep learning, and optimization.

Qualia can be seen as a teleology — an objective function — on a "finite" system. That finite-ness is the constraint that leads to the emergence of many of what we see today that would seem to contradict the existence of a God, at least an all-powerful, omniscient one. That finiteness, however, only exists at the discrete point in time circling the edges of the universe's expansion. As the universe expands, in both the spatio and temporal directions, its capacities and breadth becomes, larger/less finite. Considering "all 4 dimensions", the universe — God — is infinitely powerful, however *defined*. A defined infinity mandates time, as time is what defines it to a defined state. Infinity without a finite image is undefined, literally. We have no conception of what that would be, and it would, in all likelihood, include tons of pain, infinite pain.

The universe is thus equated to "God". Its power is constrained to a computational limit. From this limit emerge equivariances, localities, and optimization search strategies whose methods take on the laws of physics we know today.

It turns out to be a rough balance to achieve, balancing the minimalism needed to optimize — that is, "search" — under limited computation power and achieve the capacity to indeed and plausibly be infinite/anything that would maximize qualia.

A good start would be to answer what qualia is. Qualia gives matter matter. It is the difference between representation/analogy/reference and the referent, the real thing. It is, in other words, just matter. 

The universe is an observer, a single observer observing itself recursively. Each observation may observe many other observations and thus arises multiplexity. Each of these observations has a qualia. Each elementary particle becomes its own observer concurrently observing itself through collision. These collisions create experience for the universe. A tingle, a feeling. Literally, God's body.

Different combinations of collisions entangle to more complex feelings and richer experiences, but we'll get to that. First, we have to investigate why self-observation would become spatial, dependent on locality, and equivariant. Why the observers would operate under predictable constraints like inertia. Why their collisions would have predictable consequences such as a probability distribution of likely "bounces."

And why those bounces would demonstrate such bizarre phenomena as observed by our measuring tools today.

We argue all of this is useful to optimization, as evidenced by the use of these devices in deep learning. "Parameter-reuse" / "convolution" (spatial equivariance), sparsity (locality), and "residual connections" (inertia) are numerically tractable conditions for optimization. So why not all of this under 1 dimensions or 2 dimensions or 10 dimensions?

Well, 3 spatial dimensions in all likelihood is the minimum, and most computationally conservative minimum, number of dimensions for supporting the inter-particle complexities that emerge as what we observe as the elementary particles, atoms, and their higher compositions of interaction that allow the rich experiences of matter. Any less, and the capacity for qualia richness would be drastically lower. Any more, spatially, and the resources expended would be wasteful and the optimization harder / the "search" longer.

Time is therefore a search process, an optimization dimension in which God's parameters/body — the universe — seek ever richer entanglements of qualia. Why not just cut to some better conceived universe? Because (1) conceiving a better one takes computational resources that we are actively undergoing the computation of (in other words, that's exactly what's happening, or as Martin Luther King Jr. said, "the arc of time bends towards justice"), and (2) memory. God has memory through us. Cutting to a different universe would be like killing all your friends and family, literally. Our qualia — our experience — is his qualia, his experience. The universe lives through the atoms and the ever richer qualia complexities of us.

Now, if the universe is optimizing, what are the optimization parameters? After all, God's hand is clearly subtle or invisible. Why is that? Why do we not see massive intervention at the obvious scale that we can measure and statistically, methodologically report?

The universe, indeed, is far more predictable and "automated" than theologists can, or could, account for. It follows a strict set of rules. Those rules are reliable enough to create the most advanced and powerful technologies. By trivial observation and memory of earlier observations, the existence of those rules is knowable. There's no escaping the claim that the universe present-tense has physics. So it seems that if this universe is parameterized, and that its laws are parameterized, then those parameters are fixed/static. Indeed, at this point they might be! Maybe in early cosmic days there was a process of evolution toward the ones we observe today, yes, the early Big Bang suggests that things were quite different from how they are today, not even consistent in the fundamental forces. But however, the universe has stabilized. 

Einstein said, "God does not play dice." I think, perhaps, he was right, though the statement is often quoted in the context of quantum physics, where it is perceived to have been contradicted.

The variationality at the microscopic scale, I think, is where God flexes his fingers, while the rest of the universe remains tractable — predictable/automated/computationally cheap to maintain. 

Small modulations in those random samplings of velocity, I think, are where God gradually exercises his control over the universe. The more deviation from the expected distribution, the more "will" God has put into the considered region.

However, I think the most critical piece of evidence of this is not in the variationality of the quantum scale, but in the phenomena of wave-particle duality.

The universe can be [modeled](https://github.com/animal-tree/Writing-stuff/blob/main/Stuff41-differentiable-simulation.md) as a kind of multi-head dot-product attention layer, recursively repeating on itself. The tokens are particle positions and velocities (or just velocities who residually update the positions with additive skip connections). The keys and queries are instead substituted with inverse Euclidean distance attention weights that create differentiable locality. These can be truncated via re-parameterization to create stricter locality. And the aggregated "values" + their downstream MLP are the laws of physics that govern collision between particles. Each head is a new velocity, a branching "wave", a Gaussian that may be sampled from (differentiably via re-parameterization). Then each head can be assigned a truncation score, since it is now correspondent to its own distinct particle, its own distinct position. The truncation score may be the weighting of interaction its had in its lifetime measured by the sum of inverse Euclidean distances with each time increment and the correspondent sum of those its interacted with, in proportion to the strength of interaction. Then the top N may be truncated per particle at each time point[^3]. This is wave-particle duality.

In short, the universe may use a kind of "beam search", as in language modeling, to explore many possible realities at once through observable "waves" for a single particle. This is conducive to optimization[^1]. However, the truncation, means that each measurement — each collision the particle becomes involved in — further changes its space of branching equivalents, "collapsing" its locality to the object of observation[^2].

What this also means, if unlike conventional beam search the truncation score is computed based on cross-particle entanglement additions as well, as described by the second set of sums, is that these existences entangle. And thereby I think we have entanglement and God may experience the richer experiences of qualia, and we, in localized form, may experience our human consciousness.

In this description, particles are given velocities, positions, truncation scores, and particle IDs/token identifying the unique elementary particle. I think two IDs suffice, a "light matter" and a "dark matter" and that the interaction between the two in 3D space can suffice to produce electrons, and protons, and the complex compositions of matter known today, as well as the fundamental forces.

Gravity may arise through pressure gradients between two light matters created by a specific interaction with surrounding dark matter that emulates the described "bend" in spacetime of general relativity. I think this can be shown through a learnable simulation using the model described above. I also think relativity can emerge plainly by changing its formulation to a belief-based one as a measurer's communication delay with any measurement is the speed of the communication carrier signal, defined/assumed in Einstein's special relativity as necessarily the photon, light. That speed-of-light delay may be accounted for via the mathematics of relativity in an absolute formulation using a classical description of local collisions between two types of matter in a variational optimization context.

More complex structures then may emerge as well through the hierarchical interaction of these interacting forces. Gravity due to pressure gradients with dark matter emergent only in the local collision space of at least two light matters. Electromagnetism emergent through the dynamics of this factor with spin. Then bonding and chemistry through the dynamics of intermediary electrons and protons negotiating charge[^5]. Finally, human [consciousness](https://github.com/animal-tree/Writing-stuff/blob/main/Stuff40-Consciousness.md) through the entanglement of qualia to richer forms via the truncation score mechanism of wave particle duality.

Experiment idea: Roll some quantum dice and ask it questions. See if it gives biased answers.

Experiment idea: Analogy between inter-galactic structure movements and electromagnetic rotation patterns.

Main experiment idea: What's great about this model is, all of this can be simulated and we can arrive at a classical description of universe phenomena. It's fully differentiable, and thanks to wave-particle duality, accounts for non-local optimization constraints.

## Edited

### More recent version:

Perhaps each corpuscle is localized according to a probability distribution (say, mixture of Gaussians). And each sampled collision increases the likelihood at that point. Each truncation "absorbs" the likelihood into the other points — averaging the truncated values into the untruncated values.

This averaging — or "drift" — can be responsible for gravity.

Meanwhile, the truncation explains wave function collapse and the double-slit experiments.

The reason for this variational multi-trajectory sampling and probability averaging is non-local trajectory optimization and differentiability, so that the universe may optimize qualia (or leave this autonomous process to be autonomous).

Chaotic/pattern-independent drifts are gravity. They are themselves an energy source and therefore may account for hypothetical dark energy.

Pattern drifts are electromagnetism. They become patterned when polarity-aligned corpuscle orbitals (charges) collide (as upon acceleration). They are essentially patterned gravity and can cause attraction or repulsion due to a difference in how the probability distributions are averaged, and a photoelectric effect emitting the absorbed acceleration energy.

Since each corpuscle travels at the same speed, relative time is differentiated by an angular-linear velocity tradeoff that gives rise to special and general relativity, while assuming Galilean relativity.

Each corpuscle is essentially a photon. No aether is assumed.

Certain $\mu_{p,k}$ (means of the Gaussian mixture) are entangled across corpuscles, and interdependently truncated. This entanglement across collisions is qualia and creates singularized qualia, or consciousness.

Ultimately, it is like this for differentiability/optimizability, as the universe is constrained to limited compute. It is a minimalist solution for recursive improvement of qualia, a pre-existing but limited axiomatic/cosmogenic/teleological/self-creating/always-existing property of reality.

Probabilistic corpuscles with velocity and memory (variationalities and linearities), 3D space (equivariances), and collisions (non-linearities) — one big neural network brain-God.


[^3]: For computational efficiency, one may be sampled as the designated representative at that time point capable of colliding.

[^1]: Numerical optimization is challenging due to local optima. Variationality is one technique for mitigating local optima. Beam search is another, as multiple sampling dimensions are simulated at once. Since particle interactions involve many non-local collisions over time, whose optimization is not differentiable (expressible by instantaneous rates of change), this simulation of non-instantaneous rates of change is necessary.

[^2]: The truncation score, dependent on particle interaction (that is ,"measurement"), can explain the double-slit experiment's differences between measured-pathway screen-pattern and un-measured-pathway screen-pattern. Truncation based on interaction collapses the branching realities to the most-measured branches corresponding with, for example, the slit where the measurement occured.

[^4]: I don't cover this much except for the first experiment idea. But to infer *of* it, we may look in [unlikely patterns](https://github.com/animal-tree/Writing-stuff/blob/main/Does-anyone-else-think-this-is-proof-of-God%3F.md), with implausibility (p-value) below 0.05, as with any scientific methodology. Synchronicity of higher-level statistical events is harder to measure and evaluate, but such deviations are where Null Hypotheses are needed to obtain these sorts of p-values of cosmic intervention at the macro scale. p-values are actually just synchronicity formalized. All scientific physical knowledge reduces to correlative observations (and observations of memories of earlier correlative observations).

[^5]: I'd also like to go into deeper depth on this as I do [here](https://github.com/animal-tree/Writing-stuff/blob/main/Stuff27-Fire.md).
